---
title: "Introduction to `flashier`"
author: "Jason Willwerscheid"
date: "07/18/2023"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{flashier intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>",
                      fig.width = 6, fig.height = 6, warning = FALSE)
library(flashier)
library(ggplot2)
```

## Model

The empirical Bayes matrix factorization (EBMF) model puts
$$ Y = LF^T + E, $$
where $Y$ is an ${n \times p}$ data matrix, $L$ is an ${n \times K}$ matrix of "loadings" $\ell_1, \ldots, \ell_K$, $F$ is a ${p \times K}$ matrix of "factors" $f_1, \ldots, f_K$, and $E$ is an ${n \times p}$ matrix of residuals with
$$ e_{ij} \sim N(0, 1 / \tau_{ij}).$$
The model puts priors on each factor and each set of loadings:
$$\ell_k \sim g^{(\ell)}_k,\ f_k \sim g^{(f)}_k,$$
with the priors $g^{(\ell)}_k$ and $g^{(f)}_k$ assumed to belong to some families of distributions $\mathcal{G}^{(\ell)}$ and $\mathcal{G}^{(f)}$ and then estimated using the data. The default is the family of point-normal distributions:
$$ g \sim \pi_0 \delta_0 + (1 - \pi_0) N(0, \sigma^2), $$
where both $\pi_0$ and $\sigma^2$ are free parameters. To avoid over-parametrization, it is also necessary to make some assumptions about the precision parameters $\tau_{ij}$. The default assumption is that all $\tau_{ij}$s are equal:
$$ e_{ij} \sim N(0, 1 / \tau).$$

Note that when the prior families $\mathcal{G}^{(\ell)}$ and $\mathcal{G}^{(f)}$ are closed under scaling (as is typically the case), then the above model is not identifiable. In particular, the scale of the loadings $\ell_k$ is in itself meaningless, as is the scale of the factors $f_k$. We can make the factorization unique by writing
$$ Y = LDF' + E, $$
with the scales of loadings $\ell_1, \ldots, \ell_K$ and factors $f_1, \ldots, f_K$ constrained in some fashion (for example, by setting $\| \ell_k \|_2 = 1$ and $\| f_k \|_2 = 1$ for all $k$).

## Basic example

As a running example throughout the vignette, I use 1000 tests from the GTEx project, which comprise a subset of the `strong` dataset described [here](https://stephenslab.github.io/mashr/articles/eQTL_outline.html). While EBMF can "automatically" select the number of factor/loadings pairs $K$ (pairs are no longer added when they fail to yield an increase in the variational lower bound on the log likelihood, or ELBO), I cap $K$ at 5 using parameter `greedy_Kmax`:

```{r gtex.const}
data(gtex, gtex_colors)
gtex_const <- flash(gtex, greedy_Kmax = 5)
```

The returned `flash` object is a list that contains useful information about the fit. For example, the ELBO attained using the default variance structure (with all $\tau_{ij}$s equal) is

```{r gtex.output}
gtex_const$elbo
```

A few useful methods have been provided, including `fitted` (which returns $\mathbb{E} (LF')$), `residuals` (which yields the matrix of expected residuals $Y - \mathbb{E} (LF')$), and `ldf` (which gives the $LDF'$ factorization with the scaling of loadings and factors determined by parameter `type`).

A convenient `plot` method is also provided:

```{r plot.fn}
plot(gtex_const, pm_colors = gtex_colors)
```

## Variance structures

In general, the matrix of precision parameters $\tau_{ij}$ is assumed to be rank-one. That is, $\tau_{ij} = \tau^{(1)}_i \tau^{(2)}_j$, where $\tau^{(1)}$ is a $n$-vector and $\tau^{(2)}$ is a $p$-vector. This general assumption can be motivated by noting that it describes a model in which residuals are distributed $E_{ij} \sim N(0, 1)$ and the rows and columns of $Y$ have each been scaled by some constant: 
$$ \text{diag} (\tau^{(1)}) Y \text{diag} (\tau^{(2)}) = L F^T + E.$$

By assuming that all elements of $\tau^{(1)}$ are identical, and likewise for $\tau^{(2)}$, one recovers the default case where all $\tau_{ij}$s are equal. One might also assume that all elements of $\tau^{(1)}$ are identical but that the elements of $\tau^{(2)}$ vary. In this case, one obtains column-specific variances
$$ E_{ij} \sim N(0, 1 / \tau_j).$$

In `flashier`, the residual variance structure is defined by specifying the $\tau^{(n)}$s whose elements are allowed to vary. For example, row-specific variances can be fit as follows:

```{r gtex.byrow}
gtex_byrow <- flash(gtex, greedy_Kmax = 5, var_type = 1, verbose = 0)
c(const_elbo = gtex_const$elbo, byrow_elbo = gtex_byrow$elbo)
```

(I set `verbose = 0` to suppress progress updates.) Since the assumption of row-specific variances is much more flexible than the assumption of a constant residual variance (and includes it as a special case), the objective has increased considerably. The factors appear as follows:

```{r gtex.byrow.plot}
plot(gtex_byrow, pm_colors = gtex_colors, include_scree = FALSE)
```

The general rank-one case can be fit as follows.
```{r gtex.kronecker}
gtex_kronecker <- flash(gtex, greedy_Kmax = 5, var_type = c(1, 2), verbose = 0)
c(const_elbo = gtex_const$elbo, byrow_elbo = gtex_byrow$elbo, kron_elbo = gtex_kronecker$elbo)
```

It is worth noting that the precision parameters can be obtained analytically when they vary along a single dimension (rows or columns). When more than one dimension is used, they must be estimated via an alternating maximization algorithm, which is typically slower.

## Measurement error

It is possible that the data $Y$ is observed with some known error. In such a case, it might be preferable to fit the model

$$ Y = L F' + S + E, $$
where $S_{ij} \sim N(0, s^2_{ij})$ and the $s^2_{ij}$s are fixed. In other words, one might prefer to fit the model
$$ Y_{ij} \sim N \left(\sum_k \ell_{ik} f_{jk}, s^2_{ij} + 1 / \tau_{ij}\right). $$

In some cases, this model can nearly be reduced to the model described above. For example, since the `gtex` data is a matrix of $z$-scores, one might set the $s_{ij}$s equal to one. With, for example, row-specific residual variances, this yields the model
$$ Y_{ij} \sim N \left(\sum_k \ell_{ik} f_{jk}, \sigma^2_{j} + 1 \right). $$
This is almost the same as the model described in the previous section, with the important difference that residual variances cannot be less than 1. The objective will necessarily be lower than the model with arbitrary row-specific variances, but the model is arguably more correct.

```{r gtex_byrow_plus1}
gtex_byrow_plus1 <- flash(gtex, S = 1, greedy_Kmax = 5, var_type = 1, verbose = 0)
c(byrow_elbo = gtex_byrow$elbo, byrow_p1_elbo = gtex_byrow_plus1$elbo)
```

## Prior families

In `flashier,` one does not define the prior families $\mathcal{G}^{(\ell)}_k$ and $\mathcal{G}^{(f)}_k$ directly, but rather implicitly as the functions used to solve the EBNM problem
$$ \begin{gather} x_i \sim \mathcal{N} \left( \theta_i, s_i^2 \right) \\ \theta_i \sim g \in \mathcal{G}, \end{gather} $$
with the $x_i$s and $s_i$s known observations and standard errors and the $\theta_i$s unknown "means." Note that these EBNM functions must not only specify $\mathcal{G}$, but also the method for estimating $g \in \mathcal{G}$.

A number of useful EBNM functions are provided by package `ebnm` (Willwerscheid and Stephens 2021). The default setting is `ebnm_fn = ebnm_point_normal` which, as mentioned above, estimates $g$ from among the family of point-normal distributions:
$$ g \sim \pi_0 \delta_0 + (1 - \pi_0) N(0, \sigma^2). $$
More flexible families of priors are also available. For example, setting `ebnm_fn = ebnm_normal_scale_mixture` estimates $g$ from among the family of scale mixtures of normals (which contains the family of point-normal distributions), while `ebnm_unimodal_symmetric` takes $\mathcal{G}$ to be the family of all symmetric distributions that are unimodal at zero (which contains the family of scale mixtures of normals). 

```{r gtex.normalmix}
pn_time <- system.time(
  gtex_pn <- flash(
    gtex, 
    greedy_Kmax = 5, 
    verbose = 0
  )
)
normalmix_time <- system.time(
  gtex_normalmix <- flash(
    gtex, 
    greedy_Kmax = 5, 
    ebnm_fn = ebnm_normal_scale_mixture, 
    verbose = 0
  )
)
unimix_time <- system.time(
  gtex_unimix <- flash(
    gtex, 
    greedy_Kmax = 5, 
    ebnm_fn = ebnm_unimodal_symmetric, 
    verbose = 0
  )
)
c(pn_elbo = gtex_pn$elbo, smn_elbo = gtex_normalmix$elbo, symmuni_elbo = gtex_unimix$elbo)
c(pn = pn_time[3], smn = normalmix_time[3], symmuni = unimix_time[3])
```

To pass non-default arguments to `ebnm` functions, `flashier` provides the convenience function `flash_ebnm`, which simply passes its arguments to `ebnm`. For example, to fit a normal prior with both mean and variance to be estimated,
$$ g \sim N(\mu, \sigma^2), $$
one can set `ebnm_fn = flash_ebnm(prior_family = "normal", mode = "estimate")`. The latter can be especially useful for fitting a "mean" factor such as the first factor in the `gtex` example 
(which we wouldn't expect to be centered at zero, since the data is not centered):

```{r gtex.w.mean}
gtex_intercept <- flash(
  gtex, 
  greedy_Kmax = 1,
  ebnm_fn = flash_ebnm(prior_family = "normal", mode = "estimate"),
  verbose = 0
)
```

Finally, one can assign different EBNM functions to the loadings $L$ and the factors $F$. For instance, one can obtain flexible unimodal factors with normally distributed loadings as follows:

```{r gtex.nn}
gtex_diff_ebnm <- flash(
  gtex, 
  greedy_Kmax = 5, 
  ebnm_fn = c(ebnm_normal, ebnm_unimodal_symmetric),
  verbose = 0
)
```

## Backfitting

The above models are "greedily" fit by updating a first factor/loadings pair to optimize the ELBO, then adding and optimizing a second pair while leaving the first fixed, and so on. Optionally, one may choose to "backfit" a model by iteratively updating pairs one at a time until all have converged. In this way, factors that are added early on can, so to speak, use information contained in subsequent factors to improve the overall model fit. The improvement can be considerable:

```{r backfit}
bf_time <- system.time(
  gtex_bf <- flash(
    gtex, 
    greedy_Kmax = 5, 
    var_type = 1, 
    backfit = TRUE, 
    verbose = 0
  )
)
c(greedy_elbo = gtex_byrow$elbo, bf_elbo = gtex_bf$elbo)

plot(gtex_bf, pm_colors = gtex_colors, include_scree = FALSE)
```

## Sampling from the posterior

One of the list elements in the object returned by `flash` is a function that can sample from posterior distributions on loadings and factors. Take the backfitted object above as an example. To better understand which tissues are bound up with strong effects in whole blood, I would like confidence intervals for the third factor. I construct 95% confidence intervals using 200 samples.

```{r final.bf.ci}
# Use returned sampler to sample from posterior.
samp <- gtex_bf$sampler(nsamp = 200)
# Only keep factor 3.
factor3_samp <- lapply(samp, function(x) x[[2]][, 3])
# Normalize the loadings.
factor3_samp <- sapply(factor3_samp, function(x) x / max(abs(x)))
# Get 95% confidence intervals.
factor3_ci <- apply(factor3_samp, 1, quantile, c(0.025, 0.975))
```

Since the `plot` method returns a `ggplot2` object, it can be customized using `ggplot` syntax, making the addition of error bars a simple task:

```{r final.bf.ci.plot}
plot(gtex_bf, kset = 3, pm_colors = gtex_colors, include_scree = FALSE) +
  geom_errorbar(aes(ymin = factor3_ci[1, ], ymax = factor3_ci[2, ]))
```
